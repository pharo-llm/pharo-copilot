<!DOCTYPE HTML>
<html>
		<head>
	    <meta charset="UTF-8" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
	    <link rel="stylesheet" type="text/css" href="/Pharo-Copilot/files/simple.css" /> 
	    <title>ChatPharo</title>
	</head>
	<body>
	
	<div id="header">
	<div id="headerSiteTitle">
	ChatPharo Documentation
	</div>
	<div id="headerPageTitle"> 
	</div>
</div>
	<div id="menu">
	<div id="toc">
		<div id="tocItem"><a href="/Pharo-Copilot/index.html">Home</a>
		</div>
		<div id="tocItem"><a href="/Pharo-Copilot/publications.html">Publications</a>
		</div>	
		<div id="tocItem"><a href="/Pharo-Copilot/news/index.html">News</a>
		</div>
	</div>
</div>    <!--Closing menu -->


	<div id="mainContent">
		
		<div id="personalDetails">
		<div><img src="/Pharo-Copilot/files/logo.png" width="128px"></div>
	</div>

	<h1>Pharo Copilot – System Documentation</h1><h2>Overview</h2><p>Pharo Copilot is an AI-powered completion engine for the Pharo environment that supplies context-aware code suggestions through an Ollama-backed language model. It installs as a completion engine within Pharo’s settings and can be loaded via Metacello for Pharo 13 or 14 images.【F:README.md†L6-L31】</p><h2>Architecture</h2><p>The project is organized into distinct packages that separate editor integration, backend communication, configuration, and evaluation:</p><ul><li><strong>Baseline</strong>: <code>BaselineOfAIPharoCopilot</code> defines the project baseline and assets such as the Copilot logo used by the tooling.</li><li><strong>Front-End Completion Engine (<code>AI-Pharo-Copilot</code>)</strong>: Implements the completion engine, result-set builder, and logging utilities that interface with the Pharo editor.</li><li><strong>Backend & Settings (<code>AI-Pharo-Copilot-Ollama</code>)</strong>: Provides the Ollama HTTP client, model registry/specification helpers, and centralized settings for model selection, templates, logging, and auto-install behavior.</li><li><strong>Evaluator (<code>AI-Pharo-Copilot-Evaluator</code>)</strong>: Captures acceptance/rejection metrics for suggested completions and can export evaluation reports.</li></ul><p>These packages combine to deliver an asynchronous completion pipeline that gathers editor context, queries the Ollama model, and applies normalized completions back into the editor while logging telemetry for diagnostics.</p><h2>Visual Architecture</h2><pre><code>┌────────────────────────────────────────────────────────────┐│ IDE Integration Layer                                      ││ (Pharo editor hooks)                                       ││  • CoCompletionEnginePharoCopilot                          │└───────────────┬────────────────────────────────────────────┘                │ triggers completions┌───────────────▼────────────────────────────────────────────┐│ Completion Workflow Layer                                  ││ (Result building & normalization)                          ││  • CoPharoCopilotResultSetBuilder                          ││  • cleanedContentFrom:, applySuggestion:                   │└───────────────┬────────────────────────────────────────────┘                │ gathers context & builds prompts┌───────────────▼────────────────────────────────────────────┐│ Prompting & Backend Layer                                  ││ (Prompt formatting, HTTP calls, model options)             ││  • CoPromptFormatter                                       ││  • OllamaClient (generateForPrefix:suffix:context:)        │└───────────────┬────────────────────────────────────────────┘                │ returns streamed model text┌───────────────▼────────────────────────────────────────────┐│ Observability & Evaluation Layer                           ││ (Usage telemetry and feedback)                             ││  • CoCopilotLogger                                         ││  • CoSuggestionEvaluator                                   │└────────────────────────────────────────────────────────────┘</code></pre><p>The diagram shows the full lifecycle: Pharo’s editor hands control to the completion engine, which collects contextual source, formats it through the prompt formatter, and sends a fill-in-the-middle request to the Ollama backend. Responses stream back through the client, are cleaned and applied to the editor, and are recorded by both the runtime logger and the evaluator.</p><h2>Completion Pipeline</h2><ol><li><strong>Engine initialization</strong>: <code>CoCompletionEnginePharoCopilot</code> lazily instantiates a <code>CoPharoCopilotResultSetBuilder</code>, logging whether a builder already exists before serving completions.</li><li><strong>Context capture</strong>: <code>CoPharoCopilotResultSetBuilder>>buildCompletion</code> extracts the source prefix/suffix around the cursor, records metadata (cursor position, source length, prefix/suffix snippets), and dispatches an asynchronous process to fetch suggestions. An empty <code>CoResultSet</code> backed by a <code>CoCollectionFetcher</code> is returned immediately so the editor remains responsive.</li><li><strong>Class context harvesting</strong>: When building the request, <code>classContextFor:</code> safely collects the active class definition plus instance/class-side method sources to provide richer context to the model.</li><li><strong>Model request</strong>: The asynchronous worker (<code>processCompletionFor:prefix:suffix:contextInfo:</code>) logs the request, invokes <code>OllamaClient>>generateForPrefix:suffix:context:</code> to run a fill-in-the-middle prompt, and attempts to parse JSON payloads, falling back to raw text when necessary.</li><li><strong>Content normalization</strong>: <code>cleanedContentFrom:</code> strips markdown code fences and language headers to yield raw suggestion text before applying it to the editor and logging the outcome.</li><li><strong>Application</strong>: If content remains after normalization, the builder replaces the token in the editor and logs whether text was applied or skipped due to missing content or context.</li></ol><h2>Ollama Backend Integration</h2><ul><li><strong>Client</strong>: <code>OllamaClient</code> wraps REST calls to Ollama. It formats generate payloads with model name, prompt, streaming flag, optional format, and options, then normalizes responses for downstream consumption.</li><li><strong>Fill-in-the-middle prompts</strong>: <code>generateForPrefix:suffix:context:</code> expands the configured template with the editor prefix/suffix/context, temporarily injects a <code>task: 'fill-in-the-middle'</code> option, and restores prior options after the call.</li><li><strong>Model enumeration</strong>: <code>listModels</code> hits the Ollama <code>api/tags</code> endpoint to discover installed models, logging the request and response for traceability.</li></ul><h2>Configuration & Templates</h2><p><code>CopilotSettings</code> centralizes user-facing knobs such as enabling Copilot, selecting the provider/model, and managing templates and logging.</p><ul><li><strong>Auto-install flow</strong>: If the desired model is missing, <code>attemptAutoInstallForModelNamed:</code> can trigger a scripted install, refresh available models, and update the registry; failures are logged with contextual details.</li><li><strong>Model availability checks</strong>: <code>ensureSelectedModelAvailable</code> validates Ollama connectivity and confirms the chosen model exists, optionally prompting the user to install missing models.</li><li><strong>Template resolution</strong>: <code>defaultFimTemplate</code> first looks for cached per-model templates in the logs directory, then falls back to bundled templates, raising an error if none can be found.</li></ul><h2>Logging</h2><p><code>CoCopilotLogger</code> is responsible for structured logging across front-end and back-end events.</p><ul><li><strong>Storage</strong>: It initializes both a general log (<code>copilot.log</code>) and an evaluation log (<code>copilot-evaluation-log.jsonl</code>) inside a managed logs directory, creating directories/files on demand.</li><li><strong>Formatting</strong>: Values are normalized for readability (strings, JSON, collections, or printable fallbacks) and appended line by line, while errors during logging are surfaced via the Transcript.</li><li><strong>Event helpers</strong>: Convenience methods log backend events, frontend events (not shown above), or errors with optional stack traces to aid debugging.</li></ul><h2>Evaluation & Reporting</h2><p><code>CoSuggestionEvaluator</code> tracks user feedback on suggestions to surface model quality metrics.</p><ul><li><strong>Session statistics</strong>: The evaluator tracks totals, accepted/rejected counts, and per-model/context statistics initialized at startup.</li><li><strong>Acceptance rate</strong>: <code>acceptanceRate</code> computes accepted suggestions as a percentage of total suggestions.</li><li><strong>Export & reporting</strong>: Users can generate reports or export evaluation data to CSV with timestamps, actions, suggestion text, truncated context, model name, length, and optional rejection reasons for offline analysis.</li></ul><h2>Installation & Activation</h2><p>Install the baseline with Metacello and switch the Pharo completion engine to Copilot via Settings: <code>Code Browser → Code Completion → Completion Engine → Copilot</code>. Ensure the environment can reach the Ollama service and that the configured model is available (automatic installation can help resolve missing models).</p><h2>Operational Notes</h2><ul><li><strong>Asynchronous fetching</strong> keeps the UI responsive while completions are generated in background processes.</li><li><strong>Context-rich prompts</strong> use current class and method source to improve suggestion relevance.</li><li><strong>Safety fallbacks</strong> normalize model responses and protect against missing contexts or parsing errors to avoid editor disruption.</li><li><strong>Observability</strong> via structured logs and evaluation metrics helps diagnose backend issues and tune model performance.</li></ul><h2>Repository Layout & Key Packages</h2><ul><li><code>src/AI-Pharo-Copilot/</code>: Front-end editor integration, completion engine, result-set builder, and logging helper classes.</li><li><code>src/AI-Pharo-Copilot-Ollama/</code>: Ollama HTTP client, template management, model registry, and user-facing settings including auto-install support.</li><li><code>src/AI-Pharo-Copilot-Evaluator/</code>: Evaluation and reporting utilities that track accept/reject feedback and export reports/CSVs.</li><li><code>site/</code>: Static site assets (e.g., <code>copilot.svg</code>) used by the completion engine UI and settings panels.</li></ul><h2>Request Lifecycle (Step-by-Step)</h2><ol><li><strong>Editor triggers completion</strong> via <code>CoCompletionEnginePharoCopilot</code>, which creates (or reuses) a <code>CoPharoCopilotResultSetBuilder</code> and immediately returns an empty fetcher-backed result set to avoid blocking the UI.</li><li><strong>Context collection</strong> gathers prefix/suffix, cursor metadata, class/method source, and notebook context strings to construct a rich prompt payload.</li><li><strong>Backend call</strong> prepares a fill-in-the-middle request by expanding the configured template and temporarily inserting <code>task: 'fill-in-the-middle'</code> into the Ollama options before sending HTTP JSON via <code>OllamaClient>>generateForPrefix:suffix:context:</code>.</li><li><strong>Response normalization</strong> strips markdown fences, language headers, and whitespace to yield plain text snippets safe to apply in the editor.</li><li><strong>Result application</strong> inserts the suggestion into the editor when non-empty, logging success or skips when nothing usable remains after cleaning.</li><li><strong>Evaluation logging</strong> optionally records accept/reject actions into <code>copilot-evaluation-log.jsonl</code> along with context, timestamps, and model identifiers for later reporting.</li></ol><h2>Configuration Reference</h2><table><tr><th>Setting</th><th>Purpose</th><th>Location</th></tr><tr><td><strong>Provider & Model</strong></td><td>Selects the provider (Ollama) and specific model name used for completions.</td><td><code>CopilotSettings>>provider</code> / <code>selectedModel</code></td></tr><tr><td><strong>Auto-install</strong></td><td>Enables attempts to install missing models and refresh the registry automatically.</td><td><code>CopilotSettings>>attemptAutoInstallForModelNamed:</code></td></tr><tr><td><strong>Template selection</strong></td><td>Controls the fill-in-the-middle prompt template resolution (cached, bundled, or error).</td><td><code>CopilotSettings>>defaultFimTemplate</code></td></tr><tr><td><strong>Logging toggle</strong></td><td>Determines whether structured logs are written to the Copilot logs directory.</td><td><code>CopilotSettings>>loggingEnabled</code> & <code>CoCopilotLogger>>initialize</code></td></tr><tr><td><strong>Context options</strong></td><td>Governs class/method context collection for richer prompts.</td><td><code>CoPharoCopilotResultSetBuilder>>classContextFor:</code></td></tr></table><h2>Logging & Artifacts</h2><ul><li><strong>Log directory</strong>: Created on demand inside the Copilot folder; contains <code>copilot.log</code> (runtime events) and <code>copilot-evaluation-log.jsonl</code> (accept/reject telemetry).</li><li><strong>Human-readable formatting</strong>: Strings, collections, JSON, and objects are normalized to printable forms; failures are reported via Transcript to avoid silent loss of diagnostics.</li><li><strong>Evaluation exports</strong>: CSV/JSONL exports include timestamps, actions, context excerpts, model names, lengths, and optional rejection reasons for downstream analytics.</li></ul><h2>Template Flow</h2><ol><li><strong>Lookup</strong>: <code>CopilotSettings>>defaultFimTemplate</code> first checks cached per-model templates in the logs directory before falling back to bundled templates, throwing an error if none are found.</li><li><strong>Formatting</strong>: <code>CoPromptFormatter</code> fills placeholders for prefix, suffix, and optional context in the template prior to dispatching the Ollama request.</li><li><strong>Delivery</strong>: <code>OllamaClient</code> merges the formatted prompt with user options and transient FIM options, then posts to the <code>/api/generate</code> endpoint.</li></ol><h2>Troubleshooting</h2><ul><li><strong>Model unavailable</strong>: Run the built-in auto-install flow or refresh model tags using <code>CopilotSettings>>ensureSelectedModelAvailable</code> to validate connectivity and prompt for installation when missing.</li><li><strong>Empty suggestions</strong>: Confirm the model responds to fill-in-the-middle prompts and inspect <code>copilot.log</code> for normalization steps that may strip fenced code or language headers.</li><li><strong>Slow responses</strong>: Because requests are asynchronous, UI should remain responsive; investigate backend latency via Ollama logs and ensure network access to the service.</li><li><strong>Logging disabled</strong>: Verify <code>loggingEnabled</code> is set in <code>CopilotSettings</code>; logger initialization will skip writes otherwise.</li></ul><h2>Extensibility Notes</h2><ul><li><strong>Adding providers</strong>: Implement a new client analogous to <code>OllamaClient</code> and extend <code>CopilotSettings</code> to register provider names/models while reusing the prompt formatter and logging facilities.</li><li><strong>Custom templates</strong>: Drop per-model templates into the logs directory to override bundled defaults without changing code; the settings lookup will prefer cached templates.</li><li><strong>Alternative evaluation sinks</strong>: Extend <code>CoSuggestionEvaluator</code> to emit additional formats (e.g., HTTP events) leveraging the existing metrics and CSV export helpers.</li></ul>
	
	
<br>
<p>
  © 2025 Omar AbedelKader
</p>
	
	</div>
	
	</body>
	

</html>